{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.insert(1, '../input/gpt2source/gpt2-pytorch/pytorch-pretrained-BERT-master')\n! pip install ../input/gpt2source/gpt2-pytorch/pytorch-pretrained-BERT-master --upgrade\n","execution_count":1,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/gpt2source/gpt2-pytorch/pytorch-pretrained-BERT-master\nRequirement already satisfied, skipping upgrade: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert==0.6.2) (1.1.0)\nRequirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert==0.6.2) (1.16.4)\nRequirement already satisfied, skipping upgrade: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert==0.6.2) (1.9.173)\nRequirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert==0.6.2) (2.22.0)\nRequirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert==0.6.2) (4.32.1)\nRequirement already satisfied, skipping upgrade: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert==0.6.2) (2019.6.8)\nRequirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.173 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert==0.6.2) (1.12.173)\nRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert==0.6.2) (0.9.4)\nRequirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert==0.6.2) (0.2.1)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert==0.6.2) (1.24.2)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert==0.6.2) (2019.6.16)\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert==0.6.2) (3.0.4)\nRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert==0.6.2) (2.8)\nRequirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.173->boto3->pytorch-pretrained-bert==0.6.2) (2.8.0)\nRequirement already satisfied, skipping upgrade: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.173->boto3->pytorch-pretrained-bert==0.6.2) (0.14)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.173->boto3->pytorch-pretrained-bert==0.6.2) (1.12.0)\nBuilding wheels for collected packages: pytorch-pretrained-bert\n  Building wheel for pytorch-pretrained-bert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/fe/7c/95/fccc2929ca759896e080450ab3066b1dd5b6b244f3d818c636\nSuccessfully built pytorch-pretrained-bert\n\u001b[31mERROR: allennlp 0.8.4 requires awscli>=1.11.91, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.8.4 requires flaky, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.8.4 requires responses>=0.7, which is not installed.\u001b[0m\nInstalling collected packages: pytorch-pretrained-bert\n  Found existing installation: pytorch-pretrained-bert 0.6.2\n    Uninstalling pytorch-pretrained-bert-0.6.2:\n      Successfully uninstalled pytorch-pretrained-bert-0.6.2\nSuccessfully installed pytorch-pretrained-bert-0.6.2\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nimport warnings\nimport gc\n\n### CONFLICT ALERT if used the two lines below\n#package_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n#sys.path.append(package_dir)\nfrom pytorch_pretrained_bert import GPT2Tokenizer, GPT2ClassificationHeadModel\nfrom pytorch_pretrained_bert import GPT2Config\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, OpenAIAdam\nfrom pytorch_pretrained_bert import BertConfig\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nimport shutil\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines_gpt2(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n        one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0]*(max_seq_length- len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef convert_lines_both(example, bert_tokenizer, gpt2_tokenizer, bert_max_seq_length=320, gpt2_max_seq_length=220):\n    \n    bert_max_seq_length -= 2\n    gpt2_max_seq_length -= 2\n    bert_all_tokens, gpt2_all_tokens = [], []\n    #longer = 0\n    for text in tqdm(example):\n        bert_tokens_a = bert_tokenizer.tokenize(text)\n        gpt2_tokens_a = gpt2_tokenizer.tokenize(text)\n        if len(bert_tokens_a) > bert_max_seq_length:\n            bert_tokens_a = bert_tokens_a[:bert_max_seq_length]\n        if len(gpt2_tokens_a) > gpt2_max_seq_length:\n            gpt2_tokens_a = gpt2_tokens_a[:gpt2_max_seq_length]\n        bert_one_token = bert_tokenizer.convert_tokens_to_ids(bert_tokens_a) + [0]*(bert_max_seq_length- len(bert_tokens_a))\n        gpt2_one_token = gpt2_tokenizer.convert_tokens_to_ids([\"[CLS]\"]+gpt2_tokens_a+[\"[SEP]\"])+[0] * (gpt2_max_seq_length-len(gpt2_tokens_a))\n        bert_all_tokens.append(bert_one_token)\n        gpt2_all_tokens.append(gpt2_one_token) \n    return np.array(bert_all_tokens), np.array(gpt2_all_tokens)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\n#MAX_SEQUENCE_LENGTH = 220\n#MAX_SEQUENCE_LENGTH = 256\nMAX_SEQUENCE_LENGTH = 320\nSEED = 1234\nBATCH_SIZE = 32\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nbert_config = BertConfig(BERT_MODEL_PATH + 'bert_config.json')\nbert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\ngpt2_config = GPT2Config('../input/gpt2models/gpt2-models/config.json')\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained('../input/gpt2models/gpt2-models/')\n","execution_count":4,"outputs":[{"output_type":"stream","text":"CPU times: user 104 ms, sys: 24 ms, total: 128 ms\nWall time: 134 ms\n","name":"stdout"},{"output_type":"stream","text":"../input/gpt2source/gpt2-pytorch/pytorch-pretrained-BERT-master/pytorch_pretrained_bert/tokenization_gpt2.py:146: ResourceWarning: unclosed file <_io.TextIOWrapper name='../input/gpt2models/gpt2-models/vocab.json' mode='r' encoding='UTF-8'>\n  self.encoder = json.load(open(vocab_file))\n../input/gpt2source/gpt2-pytorch/pytorch-pretrained-BERT-master/pytorch_pretrained_bert/tokenization_gpt2.py:151: ResourceWarning: unclosed file <_io.TextIOWrapper name='../input/gpt2models/gpt2-models/merges.txt' mode='r' encoding='utf-8'>\n  bpe_data = open(merges_file, encoding='utf-8').read().split('\\n')[1:-1]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!mkdir ../input/working\n!mkdir ../input/working2\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nWORK_DIR  = '../input/working/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'  \nWORK_DIR  = '../input/working2/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":5,"outputs":[{"output_type":"stream","text":"Building PyTorch model from configuration: {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}\n\nConverting TensorFlow checkpoint from /kaggle/input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt\nLoading TF weight bert/embeddings/LayerNorm/beta with shape [768]\nLoading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\nLoading TF weight bert/embeddings/position_embeddings with shape [512, 768]\nLoading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\nLoading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n","name":"stdout"},{"output_type":"stream","text":"Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/pooler/dense/bias with shape [768]\nLoading TF weight bert/pooler/dense/kernel with shape [768, 768]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\nLoading TF weight cls/predictions/transform/dense/bias with shape [768]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 768]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../input/working/pytorch_model.bin\n","name":"stdout"},{"output_type":"stream","text":"Building PyTorch model from configuration: {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}\n\nConverting TensorFlow checkpoint from /kaggle/input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/bert_model.ckpt\nLoading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\nLoading TF weight bert/embeddings/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/embeddings/position_embeddings with shape [512, 1024]\nLoading TF weight bert/embeddings/token_type_embeddings with shape [2, 1024]\nLoading TF weight bert/embeddings/word_embeddings with shape [30522, 1024]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_0/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_0/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_1/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_1/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_10/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_10/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_11/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_11/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_12/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_12/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_12/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_12/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_12/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_12/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_13/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_13/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_13/output/LayerNorm/beta with shape [1024]\n","name":"stdout"},{"output_type":"stream","text":"Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_13/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_13/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_14/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_14/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_14/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_14/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_14/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_14/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_14/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_14/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_14/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_15/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_15/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_15/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_15/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_15/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_15/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_16/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_16/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_16/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_16/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_16/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_16/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_17/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_17/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_17/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_17/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_17/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_17/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_18/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_18/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_18/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_18/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_18/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_18/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_19/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_19/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_19/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_19/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_19/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_19/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [1024, 4096]\n","name":"stdout"},{"output_type":"stream","text":"Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_2/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_2/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_20/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_20/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_20/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_20/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_20/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_20/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_21/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_21/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_21/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_21/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_21/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_21/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_22/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_22/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_22/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_22/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_22/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_22/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_23/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_23/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_23/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_23/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_23/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_23/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_3/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_3/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_4/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_4/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_5/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_5/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [1024, 1024]\n","name":"stdout"},{"output_type":"stream","text":"Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_6/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_6/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_7/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_7/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_8/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_8/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [1024, 1024]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [4096]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [1024, 4096]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [1024]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [1024]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [1024]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [4096, 1024]\nLoading TF weight bert/pooler/dense/bias with shape [1024]\nLoading TF weight bert/pooler/dense/kernel with shape [1024, 1024]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [1024]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [1024]\nLoading TF weight cls/predictions/transform/dense/bias with shape [1024]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [1024, 1024]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 1024]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../input/working2/pytorch_model.bin\n","name":"stdout"},{"output_type":"stream","text":"CPU times: user 11 s, sys: 4.92 s, total: 15.9 s\nWall time: 19.9 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \n\nX_test_bert, X_test_gpt2 = convert_lines_both(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), bert_tokenizer, gpt2_tokenizer)","execution_count":6,"outputs":[{"output_type":"stream","text":"100%|██████████| 97320/97320 [02:38<00:00, 614.36it/s]\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 2min 40s, sys: 2.39 s, total: 2min 43s\nWall time: 2min 45s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test240 = X_test_bert[:,:240]\nX_test220 = X_test_bert[:,:220]\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LB-wise this is better\n\nmodel = torch.load(\"../input/fastai-bert-full-3-epoch/fastai_bert_full_3_epoch.model\")\n\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\ntest_preds = np.zeros((len(X_test240)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test240, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=256, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 256:(i + 1) * 256] = pred[:, 0].detach().cpu().squeeze().numpy()\n\nfastai_pred_0 = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'pytorch_pretrained_bert.modeling.BertForSequenceClassification' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.6/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'pytorch_pretrained_bert.modeling.BertEmbeddings' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.6/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'pytorch_pretrained_bert.modeling.BertSelfOutput' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/opt/conda/lib/python3.6/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'pytorch_pretrained_bert.modeling.BertOutput' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n 30%|███       | 116/381 [03:52<08:50,  2.00s/it]","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('../input/working', num_labels=6)\nmodel.load_state_dict(torch.load(\"../input/jin-fastai-weights/jin_fastai_fold_0.bin\"))\n\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\ntest_preds = np.zeros((len(X_test240)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test240, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 64:(i + 1) * 64] = pred[:, 0].detach().cpu().squeeze().numpy()\n\nfastai_pred_1 = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('../input/working', num_labels=7)\nmodel.load_state_dict(torch.load(\"../input/openai-models/fold_2_320_openai.bin\"))\n\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\ntest_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\nbase_320_0 = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('../input/working', num_labels=7)\nmodel.load_state_dict(torch.load(\"../input/openai-models/fold_3_320_openai.bin\"))\n\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\ntest_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\nbase_320_1 = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LARGE\n\n#BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\n#bert_config = BertConfig(BERT_MODEL_PATH + 'bert_config.json')\n#model = BertForSequenceClassification(bert_config, num_labels=7)\nmodel = BertForSequenceClassification.from_pretrained('../input/working2', num_labels=7)\n\nmodel.load_state_dict(torch.load(\"../input/bert-large-b/bert_large_fold_0_lr_2e-05_STAGE_4.bin\"))\n\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\nbs = 32\ntest_preds = np.zeros((len(X_test220)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test220, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=bs, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * bs:(i + 1) * bs] = pred[:, 0].detach().cpu().squeeze().numpy()\n\nlarge_320 = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## GPT2\n\nMAX_SEQUENCE_LENGTH = 220\n\n\n#X_test_gpt = convert_lines_gpt2(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = GPT2ClassificationHeadModel(gpt2_config, clf_dropout=0.4, n_class=8)\nmodel.load_state_dict(torch.load(\"../input/jin-gpt2-models/fold_1_220_gpt2.bin\"))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\ntest_preds = np.zeros((len(X_test_gpt)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test_gpt, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model.forward(x_batch.to(device))\n    test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ngpt_0 = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_pred = 0.15 * fastai_pred + 0.85*( 0.21*(pred_320_0 + pred_320_1 + pred_320_2 + pred_320_4) + 0.16*(pred_220_3) )\n#final_pred = 0.15*fastai_pred + 0.85*(0.2*(pred_320_0 + pred_320_1 + pred_320_2 + pred_320_3 + pred_320_4))\n\nfinal_pred = (0.2*fastai_pred_0 + 0.1*fastai_pred_1 + 0.2*(base_320_0 + base_320_1) + 0.4*large_320 + 0.2*gpt_0) / 1.3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    #'prediction': test_pred\n    'prediction': final_pred\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}