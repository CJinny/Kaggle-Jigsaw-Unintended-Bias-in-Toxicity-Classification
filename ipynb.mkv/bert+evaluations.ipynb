{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:28:42.875787Z",
     "start_time": "2019-06-26T05:28:42.871050Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:29:08.279519Z",
     "start_time": "2019-06-26T05:28:51.624157Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windywinter/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/windywinter/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/home/windywinter/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4900: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/windywinter/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n",
      "/home/windywinter/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/windywinter/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from /home/windywinter/JigSaw/input/bert-pretrained-models/uncased_L-24_H-1024_A-16/bert_model.ckpt\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 1024]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 1024]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/kernel with shape [4096, 1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/kernel with shape [4096, 1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [4096, 1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [1024]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 1024]\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Save PyTorch model to ../working2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../working2/config.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "from apex import amp\n",
    "import shutil\n",
    "device=torch.device('cuda')\n",
    "\n",
    "package_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.insert(0, package_dir_a)\n",
    "\n",
    "\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n",
    "from pytorch_pretrained_bert import OpenAIAdam\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"  # specify which GPU(s) to be used\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 220\n",
    "SEED = 1234\n",
    "EPOCHS = 1\n",
    "Data_dir=\"../input/jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "Input_dir = \"../input\"\n",
    "\n",
    "#! mkdir ../working2\n",
    "WORK_DIR = \"../working2/\"\n",
    "\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "#BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_L-12_H-768_A-12/'\n",
    "BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_L-24_H-1024_A-16/'\n",
    "#shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')\n",
    "\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "bert_config = BertConfig('../input/bert-pretrained-models/uncased_L-24_H-1024_A-16/'+'bert_config.json')\n",
    "\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)\n",
    "\n",
    "BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_L-24_H-1024_A-16/'\n",
    "\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "    BERT_MODEL_PATH + 'bert_config.json',\n",
    "    WORK_DIR + 'pytorch_model.bin')\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T04:29:25.532796Z",
     "start_time": "2019-06-24T04:29:25.336104Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 47G\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 04:30 bert_large_fold_0_epoch_1_ckpt_iteration_0.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 11:49 bert_large_fold_0_epoch_1_ckpt_iteration_10000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 05:14 bert_large_fold_0_epoch_1_ckpt_iteration_1000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 12:34 bert_large_fold_0_epoch_1_ckpt_iteration_11000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 13:19 bert_large_fold_0_epoch_1_ckpt_iteration_12000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 14:04 bert_large_fold_0_epoch_1_ckpt_iteration_13000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 14:49 bert_large_fold_0_epoch_1_ckpt_iteration_14000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 15:34 bert_large_fold_0_epoch_1_ckpt_iteration_15000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 05:57 bert_large_fold_0_epoch_1_ckpt_iteration_2000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 06:40 bert_large_fold_0_epoch_1_ckpt_iteration_3000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 07:23 bert_large_fold_0_epoch_1_ckpt_iteration_4000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 08:07 bert_large_fold_0_epoch_1_ckpt_iteration_5000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 08:50 bert_large_fold_0_epoch_1_ckpt_iteration_6000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 09:34 bert_large_fold_0_epoch_1_ckpt_iteration_7000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 10:18 bert_large_fold_0_epoch_1_ckpt_iteration_8000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 11:04 bert_large_fold_0_epoch_1_ckpt_iteration_9000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 15:44 bert_large_fold_0_epoch_2_ckpt_iteration_0.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 22:59 bert_large_fold_0_epoch_2_ckpt_iteration_10000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 16:26 bert_large_fold_0_epoch_2_ckpt_iteration_1000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 23:43 bert_large_fold_0_epoch_2_ckpt_iteration_11000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 24 00:28 bert_large_fold_0_epoch_2_ckpt_iteration_12000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 24 01:13 bert_large_fold_0_epoch_2_ckpt_iteration_13000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 24 01:57 bert_large_fold_0_epoch_2_ckpt_iteration_14000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 24 02:42 bert_large_fold_0_epoch_2_ckpt_iteration_15000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 17:09 bert_large_fold_0_epoch_2_ckpt_iteration_2000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 17:52 bert_large_fold_0_epoch_2_ckpt_iteration_3000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 18:36 bert_large_fold_0_epoch_2_ckpt_iteration_4000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 19:19 bert_large_fold_0_epoch_2_ckpt_iteration_5000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 20:02 bert_large_fold_0_epoch_2_ckpt_iteration_6000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 20:45 bert_large_fold_0_epoch_2_ckpt_iteration_7000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 21:29 bert_large_fold_0_epoch_2_ckpt_iteration_8000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 22:14 bert_large_fold_0_epoch_2_ckpt_iteration_9000.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 09:56 bert_large_fold_0_lr_2e-05_STAGE_1.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 15:36 bert_large_fold_0_lr_2e-05_STAGE_2.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 21:07 bert_large_fold_0_lr_2e-05_STAGE_3.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 24 02:44 bert_large_fold_0_lr_2e-05_STAGE_4.bin\r\n",
      "-rw-rw-r-- 1 windywinter windywinter 1.3G Jun 23 03:45 bert_large_fold_0_lr_2e-06_STAGE_1.bin\r\n"
     ]
    }
   ],
   "source": [
    "#! cd bert_large_models && ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T17:46:00.127849Z",
     "start_time": "2019-06-24T17:45:51.343056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained(\"../working2\", cache_dir=None, num_labels=7)\n",
    "\n",
    "#model.load_state_dict(torch.load('{}/bert_large_fold_{}_lr_{}_STAGE_{}.bin'.format(OUTDIR, I, LR, 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:29:08.303985Z",
     "start_time": "2019-06-26T05:29:08.284165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16945512448"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining cuda memory is: 16945512448\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda')\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.cuda.get_device_properties(device).total_memory\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.memory_allocated(device=None)\n",
    "print('remaining cuda memory is: {}'.format(torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device=None) ))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:29:08.418937Z",
     "start_time": "2019-06-26T05:29:08.305601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]>0.5\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]>0.5]\n",
    "    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n",
    "    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label]>0.5, examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n",
    "    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label]>0.5, examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:29:27.506708Z",
     "start_time": "2019-06-26T05:29:08.420572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1804874 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\n",
    "print('loaded %d records' % len(train_df))\n",
    "train_df['comment_text'] = train_df['comment_text'].astype(str) \n",
    "train_df=train_df.fillna(0)\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "y_columns=['target']\n",
    "train_df = train_df.drop(['comment_text'],axis=1)\n",
    "train_df['target']=(train_df['target']>=0.5).astype(float)\n",
    "\n",
    "with open('skf_5_splits.pkl', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "#skf = StratifiedKFold(n_splits=5, random_state=True, shuffle=True)\n",
    "#splits = list(skf.split(train_df, train_df['target']))\n",
    "\n",
    "lr=2e-5\n",
    "I = 0\n",
    "WARMUP=0.05\n",
    "SEED = 1234\n",
    "EPOCHS = 1\n",
    "\n",
    "#sequences = np.load('jin_files/bert_base_sequences_220.npy')\n",
    "sequences = np.load('jin_files/bert_large_sequences_220.npy')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)\n",
    "\n",
    "X = sequences[splits[I][0]]\n",
    "X_val = sequences[splits[I][1]]\n",
    "test_df = train_df.iloc[splits[I][1],:]\n",
    "\n",
    "weights = np.ones(len(train_df)) / 4\n",
    "weights += (train_df[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "weights += (( (train_df['target'].values >= 0.5).astype(bool).astype(np.int) +\n",
    "   (train_df[identity_columns].fillna(0).values < 0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "weights += (( (train_df['target'].values < 0.5).astype(bool).astype(np.int) +\n",
    "   (train_df[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "\n",
    "Y = np.vstack([(train_df['target'].values >= 0.5).astype(np.int), weights]).T\n",
    "Y_aux = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "Y = np.hstack((Y, Y_aux))\n",
    "\n",
    "y = Y[splits[I][0]]\n",
    "y_val = Y[splits[I][1]]\n",
    "\n",
    "del train_df, sequences, Y, Y_aux, weights, splits\n",
    "gc.collect()\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))\n",
    "del X, y, tokenizer\n",
    "gc.collect()\n",
    "\n",
    "def custom_loss(y_pred, y_true):\n",
    "    bce_loss_1 = F.binary_cross_entropy_with_logits(y_pred[:,:1].reshape(-1), y_true[:,:1].reshape(-1), weight=y_true[:,1:2].reshape(-1))   \n",
    "    N = y_true.shape[1]\n",
    "    custom_loss = bce_loss_1\n",
    "    for i in range(N-2):\n",
    "        custom_loss += F.binary_cross_entropy_with_logits(y_pred[:,1+i], y_true[:,2+i])\n",
    "    return custom_loss\n",
    "\n",
    "batch_size = 32\n",
    "accumulation_steps=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:29:27.512668Z",
     "start_time": "2019-06-26T05:29:27.508334Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## actual test size is around 0.97e-5\n",
    "N = 30000\n",
    "#N = 360975\n",
    "# original size 360975\n",
    "X_val_1 = X_val[:N,:]\n",
    "test_df_1 = test_df.iloc[:N,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:49:35.599389Z",
     "start_time": "2019-06-26T05:49:24.683324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"../working2\", cache_dir=None, num_labels=7)  \n",
    "\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_epoch_1_ckpt_iteration_7000.bin'\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_lr_2e-05_STAGE_1.bin'\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_epoch_1_ckpt_iteration_13000.bin'    # full eval, 0.9398\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_lr_2e-05_STAGE_2.bin'                # .., 0.9397\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_epoch_2_ckpt_iteration_9000.bin'     # 0.9405669107668508\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_epoch_2_ckpt_iteration_11000.bin'    # 0.9413588399955286\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_epoch_2_ckpt_iteration_13000.bin'\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_lr_2e-05_STAGE_4.bin'\n",
    "#output_model_file = 'bert_large_models/bert_large_fold_0_epoch_2_ckpt_iteration_14000.bin'\n",
    "\n",
    "output_model_file = 'bert_large_models/bert_large_fold_0_lr_2e-05_STAGE_4.bin'\n",
    "#output_model_file = 'bert_large_models/bert_large_full_epoch_2_ckpt_iteration_18000.bin'\n",
    "\n",
    "model.load_state_dict(torch.load(output_model_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:56:25.992611Z",
     "start_time": "2019-06-26T05:49:37.180652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b030fe6f3e418c8c305c899ae62c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windywinter/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#output_model_file = '../working2/pytorch_model.bin'\n",
    "\n",
    "#output_model_file = 'jin_files/bert_large_fold_2_STAGE_1.bin'\n",
    "bs = 256\n",
    "\n",
    "#model = BertForSequenceClassification(bert_config, num_labels=7)\n",
    "#model.load_state_dict(torch.load(output_model_file))\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.eval()\n",
    "valid_preds = np.zeros((len(X_val_1)))\n",
    "valid = torch.utils.data.TensorDataset(torch.tensor(X_val_1,dtype=torch.long))\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=bs, shuffle=False)\n",
    "\n",
    "tk0 = tqdm_notebook(valid_loader)\n",
    "\n",
    "\n",
    "for i,(x_batch,)  in enumerate(tk0):\n",
    "    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
    "    #pred = model(x_batch.to(device), labels=None)        # gpt2 only\n",
    "    valid_preds[i*bs:(i+1)*bs]=pred[:,0].detach().cpu().squeeze().numpy()\n",
    "    \n",
    "\n",
    "MODEL_NAME = 'model1'\n",
    "test_df_1[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T05:56:37.896974Z",
     "start_time": "2019-06-26T05:56:37.470647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.965087</td>\n",
       "      <td>0.895634</td>\n",
       "      <td>black</td>\n",
       "      <td>0.836693</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.964238</td>\n",
       "      <td>0.913892</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.863020</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.969577</td>\n",
       "      <td>0.909358</td>\n",
       "      <td>white</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.969173</td>\n",
       "      <td>0.952582</td>\n",
       "      <td>male</td>\n",
       "      <td>0.935604</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.975469</td>\n",
       "      <td>0.942295</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.938379</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.980655</td>\n",
       "      <td>0.916599</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.941023</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973881</td>\n",
       "      <td>0.954931</td>\n",
       "      <td>female</td>\n",
       "      <td>0.949210</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.981860</td>\n",
       "      <td>0.946921</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.961019</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.974303</td>\n",
       "      <td>0.973299</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "6  0.965087  0.895634                          black      0.836693   \n",
       "5  0.964238  0.913892                         muslim      0.863020   \n",
       "7  0.969577  0.909358                          white      0.875432   \n",
       "0  0.969173  0.952582                           male      0.935604   \n",
       "3  0.975469  0.942295                      christian      0.938379   \n",
       "2  0.980655  0.916599      homosexual_gay_or_lesbian      0.941023   \n",
       "1  0.973881  0.954931                         female      0.949210   \n",
       "8  0.981860  0.946921  psychiatric_or_mental_illness      0.961019   \n",
       "4  0.974303  0.973299                         jewish      0.984375   \n",
       "\n",
       "   subgroup_size  \n",
       "6            241  \n",
       "5            173  \n",
       "7            285  \n",
       "0            599  \n",
       "3            225  \n",
       "2            126  \n",
       "1            689  \n",
       "8            100  \n",
       "4             48  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9484513944662412\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df_1, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df_1, MODEL_NAME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STAGE 3: 0.9470025559495141\n",
    "\n",
    "# full 4: 0.9556051496446678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T17:51:50.339682Z",
     "start_time": "2019-06-24T17:51:50.118086Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.957011</td>\n",
       "      <td>0.918530</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.969006</td>\n",
       "      <td>0.925476</td>\n",
       "      <td>black</td>\n",
       "      <td>0.869186</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.973629</td>\n",
       "      <td>0.927555</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.906760</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.979628</td>\n",
       "      <td>0.935564</td>\n",
       "      <td>white</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.969556</td>\n",
       "      <td>0.963775</td>\n",
       "      <td>male</td>\n",
       "      <td>0.940952</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.971351</td>\n",
       "      <td>0.967060</td>\n",
       "      <td>female</td>\n",
       "      <td>0.946393</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.979676</td>\n",
       "      <td>0.961070</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.950758</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.979587</td>\n",
       "      <td>0.962650</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.986559</td>\n",
       "      <td>0.962118</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "5  0.957011  0.918530                         muslim      0.816667   \n",
       "6  0.969006  0.925476                          black      0.869186   \n",
       "2  0.973629  0.927555      homosexual_gay_or_lesbian      0.906760   \n",
       "7  0.979628  0.935564                          white      0.937500   \n",
       "0  0.969556  0.963775                           male      0.940952   \n",
       "1  0.971351  0.967060                         female      0.946393   \n",
       "8  0.979676  0.961070  psychiatric_or_mental_illness      0.950758   \n",
       "3  0.979587  0.962650                      christian      0.962500   \n",
       "4  0.986559  0.962118                         jewish      0.982456   \n",
       "\n",
       "   subgroup_size  \n",
       "5             22  \n",
       "6             59  \n",
       "2             50  \n",
       "7             96  \n",
       "0            196  \n",
       "1            241  \n",
       "8             35  \n",
       "3             95  \n",
       "4             22  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9538083010177182\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df_1, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df_1, MODEL_NAME)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV validation for Bert Large 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T00:29:17.109183Z",
     "start_time": "2019-06-24T00:29:00.367871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.961273</td>\n",
       "      <td>0.900299</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.846619</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.969783</td>\n",
       "      <td>0.888902</td>\n",
       "      <td>black</td>\n",
       "      <td>0.868668</td>\n",
       "      <td>2920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.967850</td>\n",
       "      <td>0.903271</td>\n",
       "      <td>white</td>\n",
       "      <td>0.876645</td>\n",
       "      <td>4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.964331</td>\n",
       "      <td>0.921852</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.892012</td>\n",
       "      <td>3935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954105</td>\n",
       "      <td>0.946253</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.903139</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.962050</td>\n",
       "      <td>0.956896</td>\n",
       "      <td>female</td>\n",
       "      <td>0.936402</td>\n",
       "      <td>10127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966795</td>\n",
       "      <td>0.951262</td>\n",
       "      <td>male</td>\n",
       "      <td>0.938215</td>\n",
       "      <td>8057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.953661</td>\n",
       "      <td>0.965647</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.938589</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.975468</td>\n",
       "      <td>0.940663</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.946386</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "2  0.961273  0.900299      homosexual_gay_or_lesbian      0.846619   \n",
       "6  0.969783  0.888902                          black      0.868668   \n",
       "7  0.967850  0.903271                          white      0.876645   \n",
       "5  0.964331  0.921852                         muslim      0.892012   \n",
       "4  0.954105  0.946253                         jewish      0.903139   \n",
       "1  0.962050  0.956896                         female      0.936402   \n",
       "0  0.966795  0.951262                           male      0.938215   \n",
       "3  0.953661  0.965647                      christian      0.938589   \n",
       "8  0.975468  0.940663  psychiatric_or_mental_illness      0.946386   \n",
       "\n",
       "   subgroup_size  \n",
       "2           2057  \n",
       "6           2920  \n",
       "7           4875  \n",
       "5           3935  \n",
       "4           1462  \n",
       "1          10127  \n",
       "0           8057  \n",
       "3           7065  \n",
       "8            836  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9414556767943845\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df_1, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df_1, MODEL_NAME)))\n",
    "#test_df_1.to_csv('bert_large_STAGE_4_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T17:16:12.251089Z",
     "start_time": "2019-06-23T17:16:08.743003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.958345</td>\n",
       "      <td>0.907456</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.847023</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.969497</td>\n",
       "      <td>0.889002</td>\n",
       "      <td>black</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>2920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.968384</td>\n",
       "      <td>0.900635</td>\n",
       "      <td>white</td>\n",
       "      <td>0.876086</td>\n",
       "      <td>4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.964730</td>\n",
       "      <td>0.919856</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.891030</td>\n",
       "      <td>3935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954909</td>\n",
       "      <td>0.944742</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.902340</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.961301</td>\n",
       "      <td>0.957684</td>\n",
       "      <td>female</td>\n",
       "      <td>0.936479</td>\n",
       "      <td>10127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.965941</td>\n",
       "      <td>0.951889</td>\n",
       "      <td>male</td>\n",
       "      <td>0.937689</td>\n",
       "      <td>8057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.953795</td>\n",
       "      <td>0.965578</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.938905</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.975496</td>\n",
       "      <td>0.940641</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.947146</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "2  0.958345  0.907456      homosexual_gay_or_lesbian      0.847023   \n",
       "6  0.969497  0.889002                          black      0.868132   \n",
       "7  0.968384  0.900635                          white      0.876086   \n",
       "5  0.964730  0.919856                         muslim      0.891030   \n",
       "4  0.954909  0.944742                         jewish      0.902340   \n",
       "1  0.961301  0.957684                         female      0.936479   \n",
       "0  0.965941  0.951889                           male      0.937689   \n",
       "3  0.953795  0.965578                      christian      0.938905   \n",
       "8  0.975496  0.940641  psychiatric_or_mental_illness      0.947146   \n",
       "\n",
       "   subgroup_size  \n",
       "2           2057  \n",
       "6           2920  \n",
       "7           4875  \n",
       "5           3935  \n",
       "4           1462  \n",
       "1          10127  \n",
       "0           8057  \n",
       "3           7065  \n",
       "8            836  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9413588399955286\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df_1, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df_1, MODEL_NAME)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T18:41:56.390431Z",
     "start_time": "2019-06-23T18:41:43.304875Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_df_1.to_csv('bert_large_it_9000_test_df.csv')\n",
    "#test_df_1.to_csv('bert_large_it_11000_test_df.csv')\n",
    "test_df_1.to_csv('bert_large_it_13000_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T20:25:26.146389Z",
     "start_time": "2019-06-22T20:25:25.877478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.964067</td>\n",
       "      <td>0.878860</td>\n",
       "      <td>black</td>\n",
       "      <td>0.827035</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.962007</td>\n",
       "      <td>0.882886</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.975833</td>\n",
       "      <td>0.871489</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.892774</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.944030</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.901515</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.975192</td>\n",
       "      <td>0.887770</td>\n",
       "      <td>white</td>\n",
       "      <td>0.901974</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.958766</td>\n",
       "      <td>0.946502</td>\n",
       "      <td>male</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.970638</td>\n",
       "      <td>0.944893</td>\n",
       "      <td>female</td>\n",
       "      <td>0.939925</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.974875</td>\n",
       "      <td>0.938401</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.981111</td>\n",
       "      <td>0.943953</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "6  0.964067  0.878860                          black      0.827035   \n",
       "5  0.962007  0.882886                         muslim      0.833333   \n",
       "2  0.975833  0.871489      homosexual_gay_or_lesbian      0.892774   \n",
       "8  0.944134  0.944030  psychiatric_or_mental_illness      0.901515   \n",
       "7  0.975192  0.887770                          white      0.901974   \n",
       "0  0.958766  0.946502                           male      0.920000   \n",
       "1  0.970638  0.944893                         female      0.939925   \n",
       "3  0.974875  0.938401                      christian      0.954167   \n",
       "4  0.981111  0.943953                         jewish      0.982456   \n",
       "\n",
       "   subgroup_size  \n",
       "6             59  \n",
       "5             22  \n",
       "2             50  \n",
       "8             35  \n",
       "7             96  \n",
       "0            196  \n",
       "1            241  \n",
       "3             95  \n",
       "4             22  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9367616556216991\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df_1, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df_1, MODEL_NAME)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T14:01:20.998626Z",
     "start_time": "2019-06-22T14:01:20.671539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.889027</td>\n",
       "      <td>0.713272</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.652287</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.899404</td>\n",
       "      <td>0.659274</td>\n",
       "      <td>white</td>\n",
       "      <td>0.669015</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.930414</td>\n",
       "      <td>0.613429</td>\n",
       "      <td>black</td>\n",
       "      <td>0.677137</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.929013</td>\n",
       "      <td>0.637829</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.759476</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.966795</td>\n",
       "      <td>0.595024</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.811665</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.925078</td>\n",
       "      <td>0.777396</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.826992</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.917040</td>\n",
       "      <td>0.799276</td>\n",
       "      <td>male</td>\n",
       "      <td>0.839162</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.943194</td>\n",
       "      <td>0.779749</td>\n",
       "      <td>female</td>\n",
       "      <td>0.871151</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.932647</td>\n",
       "      <td>0.845854</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.903125</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "8  0.889027  0.713272  psychiatric_or_mental_illness      0.652287   \n",
       "7  0.899404  0.659274                          white      0.669015   \n",
       "6  0.930414  0.613429                          black      0.677137   \n",
       "5  0.929013  0.637829                         muslim      0.759476   \n",
       "2  0.966795  0.595024      homosexual_gay_or_lesbian      0.811665   \n",
       "3  0.925078  0.777396                      christian      0.826992   \n",
       "0  0.917040  0.799276                           male      0.839162   \n",
       "1  0.943194  0.779749                         female      0.871151   \n",
       "4  0.932647  0.845854                         jewish      0.903125   \n",
       "\n",
       "   subgroup_size  \n",
       "8            100  \n",
       "7            285  \n",
       "6            241  \n",
       "5            173  \n",
       "2            126  \n",
       "3            225  \n",
       "0            599  \n",
       "1            689  \n",
       "4             48  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8107619830385726\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df_1, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df_1, MODEL_NAME)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T08:24:31.251709Z",
     "start_time": "2019-06-22T08:24:31.244976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T14:17:57.001981Z",
     "start_time": "2019-06-21T13:51:12.930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "power_mean(bias_metrics_df['subgroup_auc'], p=-5)\n",
    "power_mean(bias_metrics_df['bpsn_auc'], p=-5)\n",
    "power_mean(bias_metrics_df['bnsp_auc'], p=-5)\n",
    "print(roc_auc_score(test_df['target'], test_df['model1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T07:30:51.472095Z",
     "start_time": "2019-06-20T07:30:47.826010Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnsp_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>subgroup_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.939463</td>\n",
       "      <td>0.621428</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>0.698807</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.927327</td>\n",
       "      <td>0.658027</td>\n",
       "      <td>black</td>\n",
       "      <td>0.699959</td>\n",
       "      <td>2736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.930065</td>\n",
       "      <td>0.654232</td>\n",
       "      <td>white</td>\n",
       "      <td>0.717301</td>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.907243</td>\n",
       "      <td>0.735618</td>\n",
       "      <td>muslim</td>\n",
       "      <td>0.739049</td>\n",
       "      <td>3847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.859440</td>\n",
       "      <td>0.848040</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>0.794291</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.910647</td>\n",
       "      <td>0.781070</td>\n",
       "      <td>jewish</td>\n",
       "      <td>0.797108</td>\n",
       "      <td>1447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.930022</td>\n",
       "      <td>0.798668</td>\n",
       "      <td>male</td>\n",
       "      <td>0.848262</td>\n",
       "      <td>7975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.924740</td>\n",
       "      <td>0.815904</td>\n",
       "      <td>female</td>\n",
       "      <td>0.857535</td>\n",
       "      <td>10079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.914551</td>\n",
       "      <td>0.846384</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.865211</td>\n",
       "      <td>7204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnsp_auc  bpsn_auc                       subgroup  subgroup_auc  \\\n",
       "2  0.939463  0.621428      homosexual_gay_or_lesbian      0.698807   \n",
       "6  0.927327  0.658027                          black      0.699959   \n",
       "7  0.930065  0.654232                          white      0.717301   \n",
       "5  0.907243  0.735618                         muslim      0.739049   \n",
       "8  0.859440  0.848040  psychiatric_or_mental_illness      0.794291   \n",
       "4  0.910647  0.781070                         jewish      0.797108   \n",
       "0  0.930022  0.798668                           male      0.848262   \n",
       "1  0.924740  0.815904                         female      0.857535   \n",
       "3  0.914551  0.846384                      christian      0.865211   \n",
       "\n",
       "   subgroup_size  \n",
       "2           2022  \n",
       "6           2736  \n",
       "7           4744  \n",
       "5           3847  \n",
       "8            847  \n",
       "4           1447  \n",
       "0           7975  \n",
       "1          10079  \n",
       "3           7204  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8241944469617198\n"
     ]
    }
   ],
   "source": [
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T05:28:22.433007Z",
     "start_time": "2019-06-20T05:28:22.229206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
