{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 4.46 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"  # specify which GPU(s) to be used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "TEXT_COL = 'comment_text'\n",
    "EMB_PATH = '../input/crawl-300d-2M.vec'\n",
    "GLOVE_PATH = '../input/glove.840B.300d.txt'\n",
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')\n",
    "\n",
    "# train['target']= (train['target'].values>0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 360 Âµs\n"
     ]
    }
   ],
   "source": [
    "# print(train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.74 ms\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(embed_dir=EMB_PATH):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir,encoding='utf-8',errors='ignore')))\n",
    "    return embedding_index\n",
    "\n",
    "def build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n",
    "    embedding_matrix = np.zeros((max_features, 300))\n",
    "    for word, i in tqdm(word_index.items(),disable = not verbose):\n",
    "        if lower:\n",
    "            word = word.lower()\n",
    "        if i >= max_features: continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = embeddings_index[\"unknown\"]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def build_matrix(word_index, embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tokenizer\n",
      "time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gc\n",
    "\n",
    "maxlen = 220\n",
    "max_features = 300000\n",
    "embed_size = 600\n",
    "tokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n",
    "#tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print('fitting tokenizer')\n",
    "tokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\n",
    "word_index = tokenizer.word_index\n",
    "X_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\n",
    "y_train = train['target'].values\n",
    "X_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# del tokenizer\n",
    "# gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 578 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"lstm_att_toeken.pkl\",'wb') as f:\n",
    "    pickle.dump( tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:25, 13736.73it/s]\n",
      "2196018it [02:50, 12852.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings()\n",
    "embeddings_index2 = load_embeddings(GLOVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "embedding_matrix1 = build_matrix(word_index, embeddings_index)\n",
    "del embeddings_index\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.73 s\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix2 = build_matrix(word_index, embeddings_index2)\n",
    "del embeddings_index2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410047, 600)\n",
      "time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([embedding_matrix1,embedding_matrix2],axis=-1)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 442 ms\n"
     ]
    }
   ],
   "source": [
    "del embedding_matrix2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.34 ms\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import re\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import codecs\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.25]\n",
      " [0.   0.25]\n",
      " [0.   0.25]\n",
      " ...\n",
      " [0.   0.25]\n",
      " [1.   0.5 ]\n",
      " [0.   0.25]]\n",
      "time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "# Overall\n",
    "weights = np.ones((len(train),)) / 4\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n",
    "print(y_train)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adam = Adam(lr=2e-5,decay=0.01)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n",
    "\n",
    "\n",
    "\n",
    "def build_model(verbose = False, compile = True):\n",
    "    sequence_input = L.Input(shape=(maxlen,), dtype='int32')\n",
    "    embedding_layer = L.Embedding(len(word_index) + 1,\n",
    "                                600,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False)\n",
    "    x = embedding_layer(sequence_input)\n",
    "    x = L.SpatialDropout1D(0.2)(x)\n",
    "    x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n",
    "\n",
    "    att = Attention(maxlen)(x)\n",
    "    avg_pool1 = L.GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = L.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = L.concatenate([att,avg_pool1, max_pool1])\n",
    "\n",
    "    preds = L.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    aux_result = Dense(6, activation='sigmoid',name = 'aux_output')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[sequence_input], outputs=[preds,aux_result])\n",
    "#     if verbose:\n",
    "#         model.summary()\n",
    "#     if compile:\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.12 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
    "\n",
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼å§ç¬¬1æè®­ç»\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.3447 - dense_1_loss: 0.0693 - aux_output_loss: 0.1223 - val_loss: 0.2920 - val_dense_1_loss: 0.0575 - val_aux_output_loss: 0.1074\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.2923 - dense_1_loss: 0.0577 - aux_output_loss: 0.1070 - val_loss: 0.2816 - val_dense_1_loss: 0.0550 - val_aux_output_loss: 0.1050\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2829 - dense_1_loss: 0.0553 - aux_output_loss: 0.1054 - val_loss: 0.2776 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1039\n",
      "Epoch 4/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2775 - dense_1_loss: 0.0539 - aux_output_loss: 0.1046 - val_loss: 0.2751 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1034\n",
      "Epoch 5/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2737 - dense_1_loss: 0.0528 - aux_output_loss: 0.1041 - val_loss: 0.2756 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1032\n",
      "Epoch 6/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2701 - dense_1_loss: 0.0518 - aux_output_loss: 0.1038 - val_loss: 0.2742 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1031\n",
      "Epoch 7/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2669 - dense_1_loss: 0.0509 - aux_output_loss: 0.1036 - val_loss: 0.2740 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1030\n",
      "Epoch 8/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2646 - dense_1_loss: 0.0502 - aux_output_loss: 0.1034 - val_loss: 0.2756 - val_dense_1_loss: 0.0538 - val_aux_output_loss: 0.1030\n",
      "Epoch 9/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2617 - dense_1_loss: 0.0494 - aux_output_loss: 0.1033 - val_loss: 0.2735 - val_dense_1_loss: 0.0532 - val_aux_output_loss: 0.1029\n",
      "Epoch 10/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2592 - dense_1_loss: 0.0486 - aux_output_loss: 0.1031 - val_loss: 0.2744 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1034\n",
      "Epoch 11/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2574 - dense_1_loss: 0.0481 - aux_output_loss: 0.1031 - val_loss: 0.2746 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1030\n",
      "Epoch 12/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2549 - dense_1_loss: 0.0473 - aux_output_loss: 0.1030 - val_loss: 0.2769 - val_dense_1_loss: 0.0542 - val_aux_output_loss: 0.1030\n",
      "Epoch 13/100\n",
      "1624386/1624386 [==============================] - 150s 93us/step - loss: 0.2529 - dense_1_loss: 0.0467 - aux_output_loss: 0.1029 - val_loss: 0.2760 - val_dense_1_loss: 0.0539 - val_aux_output_loss: 0.1030\n",
      "Epoch 14/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2514 - dense_1_loss: 0.0463 - aux_output_loss: 0.1028 - val_loss: 0.2765 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1029\n",
      "Epoch 00014: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬2æè®­ç»\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 150s 93us/step - loss: 0.3449 - dense_1_loss: 0.0692 - aux_output_loss: 0.1228 - val_loss: 0.2905 - val_dense_1_loss: 0.0570 - val_aux_output_loss: 0.1074\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2909 - dense_1_loss: 0.0574 - aux_output_loss: 0.1068 - val_loss: 0.2820 - val_dense_1_loss: 0.0550 - val_aux_output_loss: 0.1053\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2822 - dense_1_loss: 0.0551 - aux_output_loss: 0.1052 - val_loss: 0.2775 - val_dense_1_loss: 0.0540 - val_aux_output_loss: 0.1042\n",
      "Epoch 4/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2764 - dense_1_loss: 0.0536 - aux_output_loss: 0.1044 - val_loss: 0.2760 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1039\n",
      "Epoch 5/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2730 - dense_1_loss: 0.0526 - aux_output_loss: 0.1041 - val_loss: 0.2749 - val_dense_1_loss: 0.0534 - val_aux_output_loss: 0.1036\n",
      "Epoch 6/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2691 - dense_1_loss: 0.0515 - aux_output_loss: 0.1037 - val_loss: 0.2750 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1034\n",
      "Epoch 7/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2663 - dense_1_loss: 0.0507 - aux_output_loss: 0.1036 - val_loss: 0.2753 - val_dense_1_loss: 0.0534 - val_aux_output_loss: 0.1039\n",
      "Epoch 8/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2636 - dense_1_loss: 0.0499 - aux_output_loss: 0.1034 - val_loss: 0.2739 - val_dense_1_loss: 0.0531 - val_aux_output_loss: 0.1033\n",
      "Epoch 9/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2608 - dense_1_loss: 0.0491 - aux_output_loss: 0.1032 - val_loss: 0.2745 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1033\n",
      "Epoch 10/100\n",
      "1624386/1624386 [==============================] - 153s 94us/step - loss: 0.2580 - dense_1_loss: 0.0483 - aux_output_loss: 0.1031 - val_loss: 0.2744 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1033\n",
      "Epoch 11/100\n",
      "1624386/1624386 [==============================] - 150s 93us/step - loss: 0.2563 - dense_1_loss: 0.0477 - aux_output_loss: 0.1031 - val_loss: 0.2771 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1034\n",
      "Epoch 12/100\n",
      "1624386/1624386 [==============================] - 152s 94us/step - loss: 0.2538 - dense_1_loss: 0.0470 - aux_output_loss: 0.1030 - val_loss: 0.2771 - val_dense_1_loss: 0.0542 - val_aux_output_loss: 0.1033\n",
      "Epoch 13/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2520 - dense_1_loss: 0.0465 - aux_output_loss: 0.1029 - val_loss: 0.2788 - val_dense_1_loss: 0.0546 - val_aux_output_loss: 0.1035\n",
      "Epoch 00013: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬3æè®­ç»\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.3419 - dense_1_loss: 0.0684 - aux_output_loss: 0.1223 - val_loss: 0.2908 - val_dense_1_loss: 0.0571 - val_aux_output_loss: 0.1076\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2889 - dense_1_loss: 0.0568 - aux_output_loss: 0.1065 - val_loss: 0.2815 - val_dense_1_loss: 0.0550 - val_aux_output_loss: 0.1050\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2806 - dense_1_loss: 0.0547 - aux_output_loss: 0.1050 - val_loss: 0.2772 - val_dense_1_loss: 0.0539 - val_aux_output_loss: 0.1041\n",
      "Epoch 4/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2753 - dense_1_loss: 0.0533 - aux_output_loss: 0.1042 - val_loss: 0.2774 - val_dense_1_loss: 0.0540 - val_aux_output_loss: 0.1040\n",
      "Epoch 5/100\n",
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.2719 - dense_1_loss: 0.0524 - aux_output_loss: 0.1039 - val_loss: 0.2773 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1037\n",
      "Epoch 6/100\n",
      "1624386/1624386 [==============================] - 150s 93us/step - loss: 0.2679 - dense_1_loss: 0.0512 - aux_output_loss: 0.1035 - val_loss: 0.2742 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1033\n",
      "Epoch 7/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2649 - dense_1_loss: 0.0504 - aux_output_loss: 0.1033 - val_loss: 0.2775 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1038\n",
      "Epoch 8/100\n",
      "1624386/1624386 [==============================] - 150s 93us/step - loss: 0.2625 - dense_1_loss: 0.0497 - aux_output_loss: 0.1031 - val_loss: 0.2742 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1031\n",
      "Epoch 9/100\n",
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.2595 - dense_1_loss: 0.0488 - aux_output_loss: 0.1029 - val_loss: 0.2769 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1032\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.2569 - dense_1_loss: 0.0480 - aux_output_loss: 0.1028 - val_loss: 0.2775 - val_dense_1_loss: 0.0543 - val_aux_output_loss: 0.1031\n",
      "Epoch 11/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2553 - dense_1_loss: 0.0475 - aux_output_loss: 0.1028 - val_loss: 0.2768 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1031\n",
      "Epoch 00011: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬4æè®­ç»\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.3433 - dense_1_loss: 0.0690 - aux_output_loss: 0.1217 - val_loss: 0.2916 - val_dense_1_loss: 0.0575 - val_aux_output_loss: 0.1071\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2898 - dense_1_loss: 0.0571 - aux_output_loss: 0.1065 - val_loss: 0.2911 - val_dense_1_loss: 0.0577 - val_aux_output_loss: 0.1058\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.2808 - dense_1_loss: 0.0548 - aux_output_loss: 0.1051 - val_loss: 0.2794 - val_dense_1_loss: 0.0545 - val_aux_output_loss: 0.1046\n",
      "Epoch 4/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2757 - dense_1_loss: 0.0534 - aux_output_loss: 0.1044 - val_loss: 0.2796 - val_dense_1_loss: 0.0545 - val_aux_output_loss: 0.1048\n",
      "Epoch 5/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2731 - dense_1_loss: 0.0527 - aux_output_loss: 0.1041 - val_loss: 0.2762 - val_dense_1_loss: 0.0538 - val_aux_output_loss: 0.1036\n",
      "Epoch 6/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2687 - dense_1_loss: 0.0514 - aux_output_loss: 0.1037 - val_loss: 0.2759 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1035\n",
      "Epoch 7/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2654 - dense_1_loss: 0.0504 - aux_output_loss: 0.1035 - val_loss: 0.2773 - val_dense_1_loss: 0.0542 - val_aux_output_loss: 0.1034\n",
      "Epoch 8/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2631 - dense_1_loss: 0.0498 - aux_output_loss: 0.1034 - val_loss: 0.2754 - val_dense_1_loss: 0.0536 - val_aux_output_loss: 0.1033\n",
      "Epoch 9/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2602 - dense_1_loss: 0.0489 - aux_output_loss: 0.1032 - val_loss: 0.2776 - val_dense_1_loss: 0.0542 - val_aux_output_loss: 0.1036\n",
      "Epoch 10/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2575 - dense_1_loss: 0.0481 - aux_output_loss: 0.1031 - val_loss: 0.2785 - val_dense_1_loss: 0.0546 - val_aux_output_loss: 0.1034\n",
      "Epoch 11/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2554 - dense_1_loss: 0.0475 - aux_output_loss: 0.1030 - val_loss: 0.2792 - val_dense_1_loss: 0.0548 - val_aux_output_loss: 0.1034\n",
      "Epoch 12/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2534 - dense_1_loss: 0.0469 - aux_output_loss: 0.1030 - val_loss: 0.2798 - val_dense_1_loss: 0.0549 - val_aux_output_loss: 0.1036\n",
      "Epoch 13/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2512 - dense_1_loss: 0.0462 - aux_output_loss: 0.1029 - val_loss: 0.2816 - val_dense_1_loss: 0.0555 - val_aux_output_loss: 0.1034\n",
      "Epoch 00013: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬5æè®­ç»\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.3416 - dense_1_loss: 0.0684 - aux_output_loss: 0.1221 - val_loss: 0.2900 - val_dense_1_loss: 0.0569 - val_aux_output_loss: 0.1073\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2883 - dense_1_loss: 0.0567 - aux_output_loss: 0.1063 - val_loss: 0.2800 - val_dense_1_loss: 0.0546 - val_aux_output_loss: 0.1049\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2795 - dense_1_loss: 0.0544 - aux_output_loss: 0.1048 - val_loss: 0.2773 - val_dense_1_loss: 0.0538 - val_aux_output_loss: 0.1046\n",
      "Epoch 4/100\n",
      "1624386/1624386 [==============================] - 148s 91us/step - loss: 0.2742 - dense_1_loss: 0.0530 - aux_output_loss: 0.1041 - val_loss: 0.2766 - val_dense_1_loss: 0.0538 - val_aux_output_loss: 0.1040\n",
      "Epoch 5/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2707 - dense_1_loss: 0.0520 - aux_output_loss: 0.1038 - val_loss: 0.2755 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1038\n",
      "Epoch 6/100\n",
      "1624386/1624386 [==============================] - 152s 94us/step - loss: 0.2668 - dense_1_loss: 0.0509 - aux_output_loss: 0.1035 - val_loss: 0.2728 - val_dense_1_loss: 0.0528 - val_aux_output_loss: 0.1034\n",
      "Epoch 7/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2638 - dense_1_loss: 0.0500 - aux_output_loss: 0.1033 - val_loss: 0.2751 - val_dense_1_loss: 0.0534 - val_aux_output_loss: 0.1038\n",
      "Epoch 8/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2612 - dense_1_loss: 0.0493 - aux_output_loss: 0.1031 - val_loss: 0.2760 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1036\n",
      "Epoch 9/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2581 - dense_1_loss: 0.0483 - aux_output_loss: 0.1030 - val_loss: 0.2747 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1035\n",
      "Epoch 10/100\n",
      "1624386/1624386 [==============================] - 149s 92us/step - loss: 0.2555 - dense_1_loss: 0.0476 - aux_output_loss: 0.1029 - val_loss: 0.2761 - val_dense_1_loss: 0.0538 - val_aux_output_loss: 0.1035\n",
      "Epoch 11/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2536 - dense_1_loss: 0.0470 - aux_output_loss: 0.1028 - val_loss: 0.2837 - val_dense_1_loss: 0.0560 - val_aux_output_loss: 0.1041\n",
      "Epoch 00011: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬6æè®­ç»\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.3416 - dense_1_loss: 0.0684 - aux_output_loss: 0.1222 - val_loss: 0.2934 - val_dense_1_loss: 0.0580 - val_aux_output_loss: 0.1073\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 152s 94us/step - loss: 0.2895 - dense_1_loss: 0.0570 - aux_output_loss: 0.1067 - val_loss: 0.2863 - val_dense_1_loss: 0.0564 - val_aux_output_loss: 0.1052\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2817 - dense_1_loss: 0.0550 - aux_output_loss: 0.1053 - val_loss: 0.2802 - val_dense_1_loss: 0.0549 - val_aux_output_loss: 0.1041\n",
      "Epoch 4/100\n",
      "1624386/1624386 [==============================] - 152s 94us/step - loss: 0.2754 - dense_1_loss: 0.0533 - aux_output_loss: 0.1044 - val_loss: 0.2792 - val_dense_1_loss: 0.0546 - val_aux_output_loss: 0.1038\n",
      "Epoch 5/100\n",
      "1624386/1624386 [==============================] - 150s 93us/step - loss: 0.2716 - dense_1_loss: 0.0522 - aux_output_loss: 0.1040 - val_loss: 0.2807 - val_dense_1_loss: 0.0549 - val_aux_output_loss: 0.1046\n",
      "Epoch 6/100\n",
      "1624386/1624386 [==============================] - 151s 93us/step - loss: 0.2678 - dense_1_loss: 0.0511 - aux_output_loss: 0.1037 - val_loss: 0.2774 - val_dense_1_loss: 0.0543 - val_aux_output_loss: 0.1033\n",
      "Epoch 7/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2646 - dense_1_loss: 0.0502 - aux_output_loss: 0.1034 - val_loss: 0.2769 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1032\n",
      "Epoch 8/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2623 - dense_1_loss: 0.0495 - aux_output_loss: 0.1033 - val_loss: 0.2808 - val_dense_1_loss: 0.0553 - val_aux_output_loss: 0.1034\n",
      "Epoch 9/100\n",
      "1624386/1624386 [==============================] - 150s 93us/step - loss: 0.2592 - dense_1_loss: 0.0486 - aux_output_loss: 0.1031 - val_loss: 0.2780 - val_dense_1_loss: 0.0544 - val_aux_output_loss: 0.1035\n",
      "Epoch 10/100\n",
      "1624386/1624386 [==============================] - 152s 93us/step - loss: 0.2565 - dense_1_loss: 0.0478 - aux_output_loss: 0.1030 - val_loss: 0.2800 - val_dense_1_loss: 0.0550 - val_aux_output_loss: 0.1034\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2549 - dense_1_loss: 0.0473 - aux_output_loss: 0.1030 - val_loss: 0.2815 - val_dense_1_loss: 0.0555 - val_aux_output_loss: 0.1034\n",
      "Epoch 12/100\n",
      "1624386/1624386 [==============================] - 150s 92us/step - loss: 0.2526 - dense_1_loss: 0.0467 - aux_output_loss: 0.1029 - val_loss: 0.2798 - val_dense_1_loss: 0.0550 - val_aux_output_loss: 0.1032\n",
      "Epoch 00012: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬7æè®­ç»\n",
      "Train on 1624387 samples, validate on 180487 samples\n",
      "Epoch 1/100\n",
      "1624387/1624387 [==============================] - 152s 94us/step - loss: 0.3410 - dense_1_loss: 0.0684 - aux_output_loss: 0.1214 - val_loss: 0.2876 - val_dense_1_loss: 0.0563 - val_aux_output_loss: 0.1069\n",
      "Epoch 2/100\n",
      "1624387/1624387 [==============================] - 153s 94us/step - loss: 0.2881 - dense_1_loss: 0.0567 - aux_output_loss: 0.1063 - val_loss: 0.2803 - val_dense_1_loss: 0.0547 - val_aux_output_loss: 0.1049\n",
      "Epoch 3/100\n",
      "1624387/1624387 [==============================] - 152s 94us/step - loss: 0.2795 - dense_1_loss: 0.0544 - aux_output_loss: 0.1048 - val_loss: 0.2756 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1039\n",
      "Epoch 4/100\n",
      "1624387/1624387 [==============================] - 153s 94us/step - loss: 0.2743 - dense_1_loss: 0.0530 - aux_output_loss: 0.1041 - val_loss: 0.2742 - val_dense_1_loss: 0.0531 - val_aux_output_loss: 0.1037\n",
      "Epoch 5/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2709 - dense_1_loss: 0.0521 - aux_output_loss: 0.1038 - val_loss: 0.2745 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1035\n",
      "Epoch 6/100\n",
      "1624387/1624387 [==============================] - 152s 94us/step - loss: 0.2669 - dense_1_loss: 0.0509 - aux_output_loss: 0.1034 - val_loss: 0.2807 - val_dense_1_loss: 0.0551 - val_aux_output_loss: 0.1038\n",
      "Epoch 7/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2641 - dense_1_loss: 0.0501 - aux_output_loss: 0.1032 - val_loss: 0.2783 - val_dense_1_loss: 0.0544 - val_aux_output_loss: 0.1036\n",
      "Epoch 8/100\n",
      "1624387/1624387 [==============================] - 152s 94us/step - loss: 0.2618 - dense_1_loss: 0.0495 - aux_output_loss: 0.1031 - val_loss: 0.2758 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1034\n",
      "Epoch 9/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2592 - dense_1_loss: 0.0487 - aux_output_loss: 0.1030 - val_loss: 0.2735 - val_dense_1_loss: 0.0531 - val_aux_output_loss: 0.1032\n",
      "Epoch 10/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2565 - dense_1_loss: 0.0479 - aux_output_loss: 0.1028 - val_loss: 0.2757 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1033\n",
      "Epoch 11/100\n",
      "1624387/1624387 [==============================] - 152s 93us/step - loss: 0.2551 - dense_1_loss: 0.0475 - aux_output_loss: 0.1028 - val_loss: 0.2744 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1032\n",
      "Epoch 12/100\n",
      "1624387/1624387 [==============================] - 151s 93us/step - loss: 0.2528 - dense_1_loss: 0.0468 - aux_output_loss: 0.1027 - val_loss: 0.2764 - val_dense_1_loss: 0.0540 - val_aux_output_loss: 0.1032\n",
      "Epoch 13/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2508 - dense_1_loss: 0.0462 - aux_output_loss: 0.1026 - val_loss: 0.2784 - val_dense_1_loss: 0.0545 - val_aux_output_loss: 0.1034\n",
      "Epoch 14/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2492 - dense_1_loss: 0.0457 - aux_output_loss: 0.1026 - val_loss: 0.2823 - val_dense_1_loss: 0.0557 - val_aux_output_loss: 0.1036\n",
      "Epoch 00014: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬8æè®­ç»\n",
      "Train on 1624387 samples, validate on 180487 samples\n",
      "Epoch 1/100\n",
      "1624387/1624387 [==============================] - 153s 94us/step - loss: 0.3420 - dense_1_loss: 0.0684 - aux_output_loss: 0.1224 - val_loss: 0.2919 - val_dense_1_loss: 0.0576 - val_aux_output_loss: 0.1070\n",
      "Epoch 2/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2887 - dense_1_loss: 0.0567 - aux_output_loss: 0.1066 - val_loss: 0.2819 - val_dense_1_loss: 0.0552 - val_aux_output_loss: 0.1048\n",
      "Epoch 3/100\n",
      "1624387/1624387 [==============================] - 151s 93us/step - loss: 0.2805 - dense_1_loss: 0.0546 - aux_output_loss: 0.1051 - val_loss: 0.2772 - val_dense_1_loss: 0.0541 - val_aux_output_loss: 0.1037\n",
      "Epoch 4/100\n",
      "1624387/1624387 [==============================] - 151s 93us/step - loss: 0.2751 - dense_1_loss: 0.0532 - aux_output_loss: 0.1043 - val_loss: 0.2758 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1034\n",
      "Epoch 5/100\n",
      "1624387/1624387 [==============================] - 152s 94us/step - loss: 0.2716 - dense_1_loss: 0.0522 - aux_output_loss: 0.1040 - val_loss: 0.2756 - val_dense_1_loss: 0.0537 - val_aux_output_loss: 0.1033\n",
      "Epoch 6/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2681 - dense_1_loss: 0.0512 - aux_output_loss: 0.1037 - val_loss: 0.2745 - val_dense_1_loss: 0.0534 - val_aux_output_loss: 0.1030\n",
      "Epoch 7/100\n",
      "1624387/1624387 [==============================] - 150s 93us/step - loss: 0.2649 - dense_1_loss: 0.0503 - aux_output_loss: 0.1034 - val_loss: 0.2845 - val_dense_1_loss: 0.0562 - val_aux_output_loss: 0.1041\n",
      "Epoch 8/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2623 - dense_1_loss: 0.0496 - aux_output_loss: 0.1033 - val_loss: 0.2761 - val_dense_1_loss: 0.0539 - val_aux_output_loss: 0.1032\n",
      "Epoch 9/100\n",
      "1624387/1624387 [==============================] - 150s 92us/step - loss: 0.2599 - dense_1_loss: 0.0488 - aux_output_loss: 0.1032 - val_loss: 0.2746 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1030\n",
      "Epoch 10/100\n",
      "1624387/1624387 [==============================] - 151s 93us/step - loss: 0.2572 - dense_1_loss: 0.0480 - aux_output_loss: 0.1030 - val_loss: 0.2749 - val_dense_1_loss: 0.0536 - val_aux_output_loss: 0.1028\n",
      "Epoch 11/100\n",
      "1624387/1624387 [==============================] - 153s 94us/step - loss: 0.2556 - dense_1_loss: 0.0476 - aux_output_loss: 0.1030 - val_loss: 0.2749 - val_dense_1_loss: 0.0536 - val_aux_output_loss: 0.1027\n",
      "Epoch 00011: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬9æè®­ç»\n",
      "Train on 1624388 samples, validate on 180486 samples\n",
      "Epoch 1/100\n",
      "1624388/1624388 [==============================] - 150s 92us/step - loss: 0.3414 - dense_1_loss: 0.0683 - aux_output_loss: 0.1221 - val_loss: 0.2926 - val_dense_1_loss: 0.0577 - val_aux_output_loss: 0.1074\n",
      "Epoch 2/100\n",
      "1624388/1624388 [==============================] - 151s 93us/step - loss: 0.2881 - dense_1_loss: 0.0566 - aux_output_loss: 0.1064 - val_loss: 0.2805 - val_dense_1_loss: 0.0548 - val_aux_output_loss: 0.1048\n",
      "Epoch 3/100\n",
      "1624388/1624388 [==============================] - 151s 93us/step - loss: 0.2798 - dense_1_loss: 0.0545 - aux_output_loss: 0.1049 - val_loss: 0.2768 - val_dense_1_loss: 0.0538 - val_aux_output_loss: 0.1040\n",
      "Epoch 4/100\n",
      "1624388/1624388 [==============================] - 151s 93us/step - loss: 0.2743 - dense_1_loss: 0.0530 - aux_output_loss: 0.1042 - val_loss: 0.2757 - val_dense_1_loss: 0.0536 - val_aux_output_loss: 0.1038\n",
      "Epoch 5/100\n",
      "1624388/1624388 [==============================] - 150s 92us/step - loss: 0.2706 - dense_1_loss: 0.0520 - aux_output_loss: 0.1038 - val_loss: 0.2746 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1035\n",
      "Epoch 6/100\n",
      "1624388/1624388 [==============================] - 150s 93us/step - loss: 0.2671 - dense_1_loss: 0.0510 - aux_output_loss: 0.1035 - val_loss: 0.2744 - val_dense_1_loss: 0.0533 - val_aux_output_loss: 0.1033\n",
      "Epoch 7/100\n",
      "1624388/1624388 [==============================] - 152s 93us/step - loss: 0.2640 - dense_1_loss: 0.0501 - aux_output_loss: 0.1033 - val_loss: 0.2750 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1034\n",
      "Epoch 8/100\n",
      "1624388/1624388 [==============================] - 153s 94us/step - loss: 0.2614 - dense_1_loss: 0.0493 - aux_output_loss: 0.1032 - val_loss: 0.2749 - val_dense_1_loss: 0.0535 - val_aux_output_loss: 0.1034\n",
      "Epoch 9/100\n",
      "1624388/1624388 [==============================] - 151s 93us/step - loss: 0.2588 - dense_1_loss: 0.0485 - aux_output_loss: 0.1030 - val_loss: 0.2799 - val_dense_1_loss: 0.0550 - val_aux_output_loss: 0.1035\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624388/1624388 [==============================] - 150s 92us/step - loss: 0.2567 - dense_1_loss: 0.0479 - aux_output_loss: 0.1030 - val_loss: 0.2781 - val_dense_1_loss: 0.0544 - val_aux_output_loss: 0.1035\n",
      "Epoch 11/100\n",
      "1624388/1624388 [==============================] - 152s 94us/step - loss: 0.2543 - dense_1_loss: 0.0472 - aux_output_loss: 0.1029 - val_loss: 0.2794 - val_dense_1_loss: 0.0548 - val_aux_output_loss: 0.1034\n",
      "Epoch 00011: early stopping\n",
      "å¼å§é¢æµ\n",
      "å¼å§ç¬¬10æè®­ç»\n",
      "Train on 1624388 samples, validate on 180486 samples\n",
      "Epoch 1/100\n",
      "1624388/1624388 [==============================] - 150s 93us/step - loss: 0.3430 - dense_1_loss: 0.0687 - aux_output_loss: 0.1227 - val_loss: 0.2920 - val_dense_1_loss: 0.0576 - val_aux_output_loss: 0.1071\n",
      "Epoch 2/100\n",
      "1624388/1624388 [==============================] - 150s 92us/step - loss: 0.2889 - dense_1_loss: 0.0569 - aux_output_loss: 0.1062 - val_loss: 0.2840 - val_dense_1_loss: 0.0557 - val_aux_output_loss: 0.1054\n",
      "Epoch 3/100\n",
      "1624388/1624388 [==============================] - 149s 92us/step - loss: 0.2800 - dense_1_loss: 0.0546 - aux_output_loss: 0.1048 - val_loss: 0.2801 - val_dense_1_loss: 0.0548 - val_aux_output_loss: 0.1043\n",
      "Epoch 4/100\n",
      "1624388/1624388 [==============================] - 152s 94us/step - loss: 0.2743 - dense_1_loss: 0.0530 - aux_output_loss: 0.1042 - val_loss: 0.2812 - val_dense_1_loss: 0.0552 - val_aux_output_loss: 0.1042\n",
      "Epoch 5/100\n",
      "1624388/1624388 [==============================] - 152s 94us/step - loss: 0.2706 - dense_1_loss: 0.0520 - aux_output_loss: 0.1038 - val_loss: 0.2793 - val_dense_1_loss: 0.0546 - val_aux_output_loss: 0.1040\n",
      "Epoch 6/100\n",
      "1624388/1624388 [==============================] - 150s 92us/step - loss: 0.2667 - dense_1_loss: 0.0509 - aux_output_loss: 0.1035 - val_loss: 0.2788 - val_dense_1_loss: 0.0545 - val_aux_output_loss: 0.1038\n",
      "Epoch 7/100\n",
      "1624388/1624388 [==============================] - 151s 93us/step - loss: 0.2631 - dense_1_loss: 0.0498 - aux_output_loss: 0.1032 - val_loss: 0.2780 - val_dense_1_loss: 0.0543 - val_aux_output_loss: 0.1036\n",
      "Epoch 8/100\n",
      "1624388/1624388 [==============================] - 150s 92us/step - loss: 0.2609 - dense_1_loss: 0.0492 - aux_output_loss: 0.1031 - val_loss: 0.2807 - val_dense_1_loss: 0.0551 - val_aux_output_loss: 0.1040\n",
      "Epoch 9/100\n",
      "1624388/1624388 [==============================] - 152s 93us/step - loss: 0.2578 - dense_1_loss: 0.0482 - aux_output_loss: 0.1030 - val_loss: 0.2800 - val_dense_1_loss: 0.0550 - val_aux_output_loss: 0.1036\n",
      "Epoch 10/100\n",
      "1624388/1624388 [==============================] - 151s 93us/step - loss: 0.2553 - dense_1_loss: 0.0475 - aux_output_loss: 0.1029 - val_loss: 0.2804 - val_dense_1_loss: 0.0551 - val_aux_output_loss: 0.1036\n",
      "Epoch 11/100\n",
      "1624388/1624388 [==============================] - 154s 95us/step - loss: 0.2535 - dense_1_loss: 0.0469 - aux_output_loss: 0.1028 - val_loss: 0.2818 - val_dense_1_loss: 0.0555 - val_aux_output_loss: 0.1037\n",
      "Epoch 12/100\n",
      "1624388/1624388 [==============================] - 151s 93us/step - loss: 0.2510 - dense_1_loss: 0.0462 - aux_output_loss: 0.1027 - val_loss: 0.2817 - val_dense_1_loss: 0.0554 - val_aux_output_loss: 0.1038\n",
      "Epoch 00012: early stopping\n",
      "å¼å§é¢æµ\n",
      "time: 5h 34min 19s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow  as tf\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=10,shuffle=True,random_state=4590).split(X_train,train['target']>0.5))\n",
    "\n",
    "K.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "oof_preds = np.zeros((X_train.shape[0]))\n",
    "test_preds = np.zeros((X_test.shape[0]))\n",
    "for fold in  range(10):\n",
    "    \n",
    "    print(\"å¼å§ç¬¬%dæè®­ç»\"%(fold+1))\n",
    " \n",
    "    clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "                   step_size=300., mode='exp_range',\n",
    "                   gamma=0.99994)\n",
    "\n",
    "\n",
    "    \n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    ckpt = ModelCheckpoint('lstm_att_%d_fold.hdf5'%(fold+1), save_best_only = True,save_weights_only=True)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    model = build_model()\n",
    "    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n",
    "    model.fit([X_train[tr_ind]],\n",
    "        [y_train[tr_ind],y_aux_train[tr_ind]],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=([X_train[val_ind]], [y_train[val_ind],y_aux_train[val_ind]]),\n",
    "        callbacks = [clr,es,ckpt])\n",
    "    print(\"å¼å§é¢æµ\")\n",
    "    oof_preds[val_ind] += model.predict(X_train[val_ind])[0].flatten()\n",
    "    test_preds += model.predict(X_test)[0].flatten()\n",
    "    \n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "test_preds /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.97 ms\n"
     ]
    }
   ],
   "source": [
    "oof_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.09 ms\n"
     ]
    }
   ],
   "source": [
    "y_train[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619367546469174"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 783 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train[:,0]>0.5,oof_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = test_preds\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.head()\n",
    "#%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
